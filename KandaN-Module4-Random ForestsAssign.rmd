---
output:
  word_document: default
  html_document: default
---
# Module 4: Assignment 
## Kanda, Nancy
### Random Forests

```{r,include = FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(gridExtra)
library(vip)
library(ranger)
```

Loading data
```{r}
drug = read_csv("drug_data.csv")
```

The columns do not have names, so we’ll supply names via the names function. Comment out the str command before knitting your completed work (to save space) :)
```{r}
names(drug) = c("ID", "Age", "Gender", "Education", "Country", "Ethnicity",
"Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive",
"SS", "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis",
"Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh",
"LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA")
#str(drug)
```

change all CL0 and CL1 values to “No” and CL2, CL3, CL4, CL5, and CL6 values to “Yes”. CL0 and CL1 imply the drug was never used or used over a decade ago. CL2 through CL6 imply more recent drug use. The code below finds any CL0, CL1, etc. values in the data frame and replaces them with the appropriate “No” or “Yes”.

```{r}
drug[drug == "CL0"] = "No"
drug[drug == "CL1"] = "No"
drug[drug == "CL2"] = "Yes"
drug[drug == "CL3"] = "Yes"
drug[drug == "CL4"] = "Yes"
drug[drug == "CL5"] = "Yes"
drug[drug == "CL6"] = "Yes"
```


```{r}
drug_clean = drug %>% mutate_at(vars(Age:Ethnicity), funs(as_factor)) %>%
  mutate(Age = factor(Age, labels = c("18_24", "25_34", "35_44", "45_54", "55_64", "65_"))) %>%
  mutate(Gender = factor(Gender, labels = c("Male", "Female"))) %>%
  mutate(Education = factor(Education, labels = c("Under16", "At16", "At17", "At18", "SomeCollege", "ProfessionalCert", "Bachelors", "Masters", "Doctorate"))) %>%
  mutate(Country = factor(Country, labels = c("USA", "NewZealand", "Other", "Australia", "Ireland","Canada","UK"))) %>%
  mutate(Ethnicity = factor(Ethnicity, labels = c("Black", "Asian", "White", "White/Black", "Other", "White/Asian", "Black/Asian"))) %>%
  mutate_at(vars(Alcohol:VSA), funs(as_factor)) %>%
  select(-ID)
```
```{r}
#str(drug_clean)
```

```{r}
drug_clean = drug_clean %>% select(!(Alcohol:Mushrooms)) %>% select(!(Semer:VSA))
names(drug_clean)
```
### Task 1: Check for missing data in our “drug_clean” dataframe. Is there any missingness? If so, identify and implement a reasonable strategy to deal with the missingness.
```{r}
summary(drug_clean)
```

  + There are no missing values in the dataset.
  
### Task 2: Split the dataset into training (70%) and testing (30%) sets. Use a set.seed of 1234. Stratify by the “Nicotine” variable.
```{r}
#Split the dataset
set.seed(1234) 
drug_split = initial_split(drug_clean, prop = 0.7, strata = Nicotine)
train = training(drug_split)
test = testing(drug_split)
```

  + The Train set has 1318 observations and test has 567 Observations.

### Task 3: Create appropriate visualizations (12 in all) to examine the relationships between each variable and “Nicotine”. Use grid.arrange (from the gridExtra package) to organize these visuals (perhaps in groups of four visualizations?). Comment on the relationship between each variable and “Nicotine”.

Visualization  
```{r}
p1 = ggplot(train, aes(x = Age, fill = Nicotine)) + geom_bar(position = "fill")+ theme(axis.text.x = element_text(angle = 90,vjust =0.5, hjust=1))
p2 = ggplot(train, aes(x = Gender, fill = Nicotine)) + geom_bar(position = "fill")+ theme(axis.text.x = element_text(angle = 90,vjust =0.5, hjust=1))
p3 = ggplot(train, aes(x = Education, fill = Nicotine)) + geom_bar(position = "fill")+ theme(axis.text.x = element_text(angle = 90,vjust =0.5, hjust=1))
p4 = ggplot(train, aes(x = Country, fill = Nicotine)) + geom_bar(position = "fill")+theme(axis.text.x = element_text(angle = 90,vjust =0.5, hjust=1))
grid.arrange(p1,p2,p3,p4)
```

```{r}
ggplot(train, aes(x = Ethnicity, fill = Nicotine)) + geom_bar(position = "fill")
```

From the visualizations above:-

   + **Age** variable have some effect on Nicotine consumption, participant between 18 and 24 are using Nicotine the most.
   + **Gender** seems to have little to no effect on consumption use
   + **Education** has some effect too, surprisingly most users have some college
   + **Country** have effect towards Nicotine consumption. Ireland has the most Nicotine Users.
   + **Ethnicity** Does have effect towards Nicotine Use. Whites have the most Nicotine users.  

```{r}
p1 = ggplot(train, aes(x = Nicotine, y = Nscore)) + geom_boxplot()
p2 = ggplot(train, aes(x = Nicotine, y = Escore)) + geom_boxplot()
p3 = ggplot(train, aes(x = Nicotine, y = Oscore)) + geom_boxplot()
p4 = ggplot(train, aes(x = Nicotine, y = Ascore)) + geom_boxplot()
grid.arrange(p1,p2,p3,p4, ncol = 2)
```
```{r}
  p1 = ggplot(train, aes(x = Nicotine, y = Cscore)) + geom_boxplot()
p2 = ggplot(train, aes(x = Nicotine, y = Impulsive)) + geom_boxplot()
p3 = ggplot(train, aes(x = Nicotine, y = SS)) + geom_boxplot()
grid.arrange(p1,p2,p3, ncol = 2)
```

  From the boxplots above:

  + **Nscore**, **Escore** and **Ascore** seems to have no effect on Nicotine consumption, while **Cscore** and **Oscore** have some effect on Nicotine consumption
  + **Impulsive** aslo  have some effect on Nicotine consumption
  + **SS** (sensation seeing),seems to have the highest effect on Nicotine consumption.
  
  
### Task 4: Create a random forest model on the training set to predict Nicotine using all of the variables in the dataset. You 5-fold, k-fold cross-validation (random number seed of 123 for the folds). Allow R to select mtry values between 2 and 8 and min_n values between 5 and 20. Use 10 levels in your “grid_regular” function. Set a random number seed of 123 for the tune_grid function. Use 100 trees. 

#### Visualize the relationships between parameters and performance metrics.

Set folds for cross-validation  
```{r}
set.seed(123)
rf_folds = vfold_cv(train, v = 5)
```

Random forest 
```{r}
drug_recipe = recipe(Nicotine ~., train) %>%
  step_dummy(all_nominal(), -all_outcomes())

rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 100) %>% 
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("classification")

drug_wflow = 
  workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(drug_recipe)

rf_grid = grid_regular(
  mtry(range = c(2, 8)), 
  min_n(range = c(5, 20)), 
  levels = 10
)

set.seed(123)
rf_res_tuned = tune_grid(
  drug_wflow,
  resamples = rf_folds,
  grid = rf_grid
)
```

Visualize relationship between parameters and performance metrics.
```{r}
rf_res_tuned %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```

  + From the visualization above, the relationship between parameters and performance metrics is a consistent across all the different values.

```{r}
rf_res_tuned %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "Accuracy")
```

  + From this visualization the best combination looks to be an mtry value of about 6 and with a min_n value of 16.

### Task 5: Use the best mtry and min_n values from Task 4 to finalize the workflow and fit the model to training set. Examine variable importance. What variables are most important in this model? (Hint: Refer back to the dataset’s webpage if you need clarification as to meaning of any variables).

```{r}
best_rf = select_best(rf_res_tuned, "accuracy")

final_rf = finalize_workflow(
  drug_wflow,
  best_rf
)

final_rf
```

```{r}
#fit the finalized workflow to the training data
final_rf_fit = fit(final_rf, train)
```

Check the variable importance
```{r}
final_rf_fit %>% pull_workflow_fit() %>% vip(geom = "point")
```

The variables that are most importance include:

  + SS (sensation seeing measured by ImpSS)
  + OScore (NEO-FFI-R Openness to experience)
  + Country_UK (Current country of residence of participant is UK)
  + Age_X45_54( participants age between 45 and 54)
  
### Task 6: How does the model perform on the training and testing sets?

#### Predictions on the train
```{r}
trainpredrf = predict(final_rf_fit, train)
```

Confusion matrix
```{r}
confusionMatrix(trainpredrf$.pred_class, train$Nicotine, 
                positive = "Yes")
```
  + The accuracy of the model on the training set is 0.9165 

#### Predictions on test
```{r}
testpredrf = predict(final_rf_fit, test)
```

Confusion matrix
```{r}
confusionMatrix(testpredrf$.pred_class, test$Nicotine, 
                positive = "Yes")
```
  + The accuracy of the model on the testing set is 0.6966

### Task 7: Comment on how this model might be used in the “real-world.” Would you recommend this model for real-world use? What if any concerns would you have about using the model?

  + The model can be used to evaluate risk to be a Nicotine consumer. 
  + The performance is not consistence, therefore would not recommend this model. The Accuracy is 0.92 on the training set and 0.70 on the testing set. There is a difference between the two. This set of parameters leads to a model that is likely to overfit.
  + With this model we have to consider if we missed the prediction on people who don't consume nicotine or the prediction on people who are actually Nicotine consumer.It is important that we predict accurately people we consume nicotine.






